{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "#from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file):\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = f1_score(y_true, np.round(y_pred))\n",
    "    return 'f1_err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(tn, fp, fn, tp):\n",
    "    print(\"    \",\"True\", \"False\")\n",
    "    print(\"True \", \" \", tp, \"  \", fp)\n",
    "    print(\"False\", \" \",fn,\"  \", tn)\n",
    "    print(\"_______________________________________\")\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________\n",
      "______________Training_________________\n",
      "F1 score 0.9973190348525469\n",
      "     True False\n",
      "True    372    2\n",
      "False   0    408\n",
      "_______________________________________\n",
      "---------------------------------------\n",
      "_______________Testing_________________\n",
      "F1 score 0.5393258426966292\n",
      "     True False\n",
      "True    24    0\n",
      "False   41    207\n",
      "_______________________________________\n",
      "---------------------------------------\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train = read_csv('output/\\lstm/\\CompleteIndexesWeeklyTrainLSTM30_0.csv')\n",
    "X_test = read_csv('output/\\lstm/\\/CompleteIndexesWeeklyTestLSTM30_0.csv')\n",
    "X_train = X_train.reindex(sorted(X_train.columns), axis=1)\n",
    "X_test = X_test.reindex(sorted(X_test.columns), axis=1)['2015-01-02':'2020-03-13']\n",
    "\n",
    "y = pd.read_csv('input/\\sp500_target_regimes.csv', parse_dates=True)\n",
    "y.index = y['date'].values\n",
    "y = y[['regime']]\n",
    "y = y['regime']=='BEAR'\n",
    "y = pd.DataFrame (y, columns = ['regime'])\n",
    "\n",
    "y_train = y.loc['2000-01-01':'2015-01-01']\n",
    "y_test = y.loc['2015-01-02':'2020-03-13']\n",
    "\n",
    "best_params = {'colsample_bytree': 0.4, \n",
    "               'gamma': 0,\n",
    "               'learning_rate': 0.1, \n",
    "               'max_delta_step': 2,\n",
    "               'max_depth': 1,\n",
    "               'min_child_weight': 3,\n",
    "               'n_estimators': 1800,\n",
    "               'reg_alpha': 0.1,\n",
    "               'scale_pos_weight': 80,\n",
    "               'subsample': 0.7}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                  eval_metric = 'auc',\n",
    "                                  learning_rate=best_params['learning_rate'], \n",
    "                                  n_estimators=best_params['n_estimators'], \n",
    "                                  min_child_weight=best_params['min_child_weight'], \n",
    "                                  gamma=best_params['gamma'],\n",
    "                                  max_delta_step=best_params['max_delta_step'],\n",
    "                                  max_depth=best_params['max_depth'],\n",
    "                                  subsample=best_params['subsample'],\n",
    "                                  scale_pos_weight=best_params['scale_pos_weight'],\n",
    "                                  colsample_bytree=best_params['colsample_bytree'],\n",
    "                                  reg_alpha=best_params['reg_alpha']\n",
    "                              )\n",
    "model = xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"_______________________________________\")\n",
    "print(\"______________Training_________________\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f_score = 2*precision*recall/(precision+recall)  \n",
    "print(\"F1 score\", f_score)\n",
    "print_confusion_matrix(tn, fp, fn, tp)\n",
    "print(\"_______________Testing_________________\")\n",
    "y_pred = model.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f_score = 2*precision*recall/(precision+recall)\n",
    "print(\"F1 score\", f_score)\n",
    "print_confusion_matrix(tn, fp, fn, tp)\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(val, pos):\n",
    "    if val == 0:\n",
    "        return \"BULL\"\n",
    "    if val == 1:\n",
    "        return \"BEAR\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(real, imputed):\n",
    "    fig, ax = plt.subplots(figsize=(20,4))\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(50))\n",
    "    ax.plot(real.index, real, '-', label=\"Real Data\", color='blue')\n",
    "    ax.plot(real.index, imputed, '-', label=\"Imputed Data\", color='red')\n",
    "    ax.legend(['Real Data', 'Predicted Data'])\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(format))\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_multiple_times(best_params, n=30):\n",
    "    result_dict = {}\n",
    "    result_dict= {'f1':0, 'tn':0, 'fp':0, 'fn':0, 'tp':0, 'auc':0, 'best_f1':0, 'best_model':None, 'best_f1':0, 'best_tn':0, 'best_fp':0, 'best_fn':0, 'best_tp':0}\n",
    "    list_f1 = []\n",
    "\n",
    "\n",
    "    for i in range(0,n):\n",
    "        y = pd.read_csv('input/\\sp500_target_regimes.csv', parse_dates=True)\n",
    "        y.index = y['date'].values\n",
    "        y = y[['regime']]\n",
    "        y = y['regime']=='BEAR'\n",
    "        y = pd.DataFrame (y, columns = ['regime'])\n",
    "\n",
    "        y_train = y.loc['2000-01-01':'2015-01-01']\n",
    "        y_test = y.loc['2015-01-02':]\n",
    "        X_train = read_csv('output/\\lstm/\\CompleteIndexesWeeklyTrainLSTM30_'+str(i)+'.csv')\n",
    "        X_test = read_csv('output/\\lstm/\\CompleteIndexesWeeklyTestLSTM30_'+str(i)+'.csv')['2015-01-02':'2020-03-13']\n",
    "        balance = np.sqrt(len(y_train[y_train['regime']==False])/len(y_train[y_train['regime']==True]))\n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                          eval_metric = 'auc',\n",
    "                                          learning_rate=best_params['learning_rate'], \n",
    "                                          n_estimators=best_params['n_estimators'], \n",
    "                                          min_child_weight=best_params['min_child_weight'], \n",
    "                                          gamma=best_params['gamma'],\n",
    "                                          max_delta_step=best_params['max_delta_step'],\n",
    "                                          max_depth=best_params['max_depth'],\n",
    "                                          subsample=best_params['subsample'],\n",
    "                                          scale_pos_weight=best_params['scale_pos_weight'],\n",
    "                                          colsample_bytree=best_params['colsample_bytree'],\n",
    "                                          reg_alpha=best_params['reg_alpha']\n",
    "                                      )\n",
    "\n",
    "        model = xgb_model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f_score = (2*precision*recall/(precision+recall))\n",
    "        print('Round', i, \"f1\", f_score)\n",
    "        list_f1.append(f_score)\n",
    "        result_dict['f1'] += f_score\n",
    "        result_dict['tn'] += tn\n",
    "        result_dict['fp'] += fp\n",
    "        result_dict['fn'] += fn\n",
    "        result_dict['tp'] += tp\n",
    "        if result_dict['best_f1'] < f_score:\n",
    "            result_dict['best_f1'] = f_score\n",
    "            result_dict['best_model'] = model\n",
    "            result_dict['best_tn'] = tn\n",
    "            result_dict['best_fp'] = fp\n",
    "            result_dict['best_fn'] = fn\n",
    "            result_dict['best_tp'] = tp\n",
    "        \n",
    "\n",
    "    #plot bear bull imputed and real\n",
    "    #plot(y_test, y_pred)\n",
    "    # feature importance\n",
    "    #data = pd.DataFrame(data=xgb_model.feature_importances_, index=X_train.columns, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "    #data.plot(kind='barh', title=\"Predicted v. Real\", figsize=(10,10))\n",
    "    print(\"STD f1\", np.std(list_f1, axis=0))\n",
    "    result_dict['f1'] = result_dict['f1']/n\n",
    "    result_dict['tn'] = result_dict['tn']/n\n",
    "    result_dict['fp'] = result_dict['fp']/n\n",
    "    result_dict['fn'] = result_dict['fn']/n\n",
    "    result_dict['tp'] = result_dict['tp']/n\n",
    "        \n",
    "    return result_dict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 f1 0.5111111111111112\n",
      "Round 1 f1 0.18666666666666665\n",
      "Round 2 f1 0.1917808219178082\n",
      "Round 3 f1 0.4\n",
      "Round 4 f1 0.11594202898550725\n",
      "Round 5 f1 0.28571428571428575\n",
      "Round 6 f1 0.3373493975903615\n",
      "Round 7 f1 0.16901408450704225\n",
      "Round 8 f1 0.059701492537313446\n",
      "Round 9 f1 0.30952380952380953\n",
      "Round 10 f1 0.08823529411764706\n",
      "Round 11 f1 0.030303030303030307\n",
      "Round 12 f1 0.3500000000000001\n",
      "Round 13 f1 0.18666666666666665\n",
      "Round 14 f1 0.10810810810810811\n",
      "Round 15 f1 0.5416666666666666\n",
      "Round 16 f1 0.5473684210526316\n",
      "Round 17 f1 0.26315789473684215\n",
      "Round 18 f1 0.2650602409638554\n",
      "Round 19 f1 0.5\n",
      "Round 20 f1 0.33333333333333337\n",
      "Round 21 f1 0.1917808219178082\n",
      "Round 22 f1 0.16901408450704225\n",
      "Round 23 f1 0.21917808219178084\n",
      "Round 24 f1 0.4901960784313726\n",
      "Round 25 f1 0.5111111111111112\n",
      "Round 26 f1 0.36585365853658536\n",
      "Round 27 f1 0.2894736842105263\n",
      "Round 28 f1 0.46464646464646464\n",
      "Round 29 f1 0.4545454545454545\n"
     ]
    }
   ],
   "source": [
    "result_dict = run_experiment_multiple_times(best_params, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.29788342648669447,\n",
       " 'tn': 203.96666666666667,\n",
       " 'fp': 3.033333333333333,\n",
       " 'fn': 52.233333333333334,\n",
       " 'tp': 12.766666666666667,\n",
       " 'auc': 0,\n",
       " 'best_f1': 0.5473684210526316,\n",
       " 'best_model': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.4, eval_metric='auc',\n",
       "               gamma=0, gpu_id=-1, importance_type='gain',\n",
       "               interaction_constraints='', learning_rate=0.1, max_delta_step=2,\n",
       "               max_depth=1, min_child_weight=3, missing=nan,\n",
       "               monotone_constraints='()', n_estimators=1800, n_jobs=0,\n",
       "               num_parallel_tree=1, random_state=0, reg_alpha=0.1, reg_lambda=1,\n",
       "               scale_pos_weight=80, subsample=0.7, tree_method='exact',\n",
       "               validate_parameters=1, verbosity=None),\n",
       " 'best_tn': 203,\n",
       " 'best_fp': 4,\n",
       " 'best_fn': 39,\n",
       " 'best_tp': 26}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score 0.29788342648669447\n",
      "     True False\n",
      "True    12.766666666666667    3.033333333333333\n",
      "False   52.233333333333334    203.96666666666667\n",
      "_______________________________________\n",
      "---------------------------------------\n",
      "_______________________________________________________________________________\n",
      "Best F1 Score 0.5473684210526316\n",
      "     True False\n",
      "True    26    4\n",
      "False   39    203\n",
      "_______________________________________\n",
      "---------------------------------------\n",
      "_______________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Average F1 Score\", result_dict['f1'])\n",
    "print_confusion_matrix(result_dict['tn'], result_dict['fp'], result_dict['fn'], result_dict['tp'])\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(\"Best F1 Score\", result_dict['best_f1'])\n",
    "print_confusion_matrix(result_dict['best_tn'], result_dict['best_fp'], result_dict['best_fn'], result_dict['best_tp'])\n",
    "print(\"_______________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
