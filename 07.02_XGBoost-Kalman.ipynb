{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "#from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file):\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = f1_score(y_true, np.round(y_pred))\n",
    "    return 'f1_err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(tn, fp, fn, tp):\n",
    "    print(\"    \",\"True\", \"False\")\n",
    "    print(\"True \", \" \", tp, \"  \", fp)\n",
    "    print(\"False\", \" \",fn,\"  \", tn)\n",
    "    print(\"_______________________________________\")\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.6, 'gamma': 1, 'learning_rate': 0.0001, 'max_delta_step': 3, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 40, 'reg_alpha': 1e-05, 'scale_pos_weight': 15, 'subsample': 0.9} 0.6307692307692307\n",
      "_______________________________________\n",
      "______________Training_________________\n",
      "F1 score 0.9575289575289575\n",
      "     True False\n",
      "True    372    33\n",
      "False   0    377\n",
      "_______________________________________\n",
      "---------------------------------------\n",
      "_______________Testing_________________\n",
      "F1 score 0.857142857142857\n",
      "     True False\n",
      "True    51    3\n",
      "False   14    204\n",
      "_______________________________________\n",
      "---------------------------------------\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train = read_csv('output/\\CompleteIndexesWeeklyTrainKalman.csv')\n",
    "X_test = read_csv('output/\\CompleteIndexesWeeklyTestKalman.csv')['2015-01-02':'2020-03-13']\n",
    "\n",
    "y = pd.read_csv('input/\\sp500_target_regimes.csv', parse_dates=True)\n",
    "y.index = y['date'].values\n",
    "y = y[['regime']]\n",
    "y = y['regime']=='BEAR'\n",
    "y = pd.DataFrame (y, columns = ['regime'])\n",
    "\n",
    "y_train = y.loc['2000-01-01':'2015-01-01']\n",
    "y_test = y.loc['2015-01-02':]\n",
    "\n",
    "#print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "balance = np.sqrt(len(y_train[y_train['regime']==False])/len(y_train[y_train['regime']==True]))\n",
    "param_test1 = {\n",
    "    'learning_rate':[1e-4],\n",
    "    'n_estimators':[40],\n",
    "    'max_depth':[2],\n",
    "    'max_delta_step':[3],\n",
    "    'subsample':[0.9],\n",
    "    'min_child_weight':[5],\n",
    "    'gamma': [1],\n",
    "    'scale_pos_weight':[15],\n",
    "    'colsample_bytree':[0.6],\n",
    "    'reg_alpha':[1e-5],\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "balance = np.sqrt(len(y_train[y_train['regime']==False])/len(y_train[y_train['regime']==True]))\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(objective=\"binary:logistic\", \n",
    "                                                      ), \n",
    "                        param_grid = param_test1, \n",
    "                        #scoring='f1',\n",
    "                        n_jobs=4,\n",
    "                        cv=tscv)\n",
    "\n",
    "gsearch1.fit(X_train, y_train)#, eval_metric=f1_eval)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "\n",
    "y_train_pred = gsearch1.predict(X_train)\n",
    "print(\"_______________________________________\")\n",
    "print(\"______________Training_________________\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f_score = 2*precision*recall/(precision+recall)  \n",
    "print(\"F1 score\", f_score)\n",
    "print_confusion_matrix(tn, fp, fn, tp)\n",
    "print(\"_______________Testing_________________\")\n",
    "y_pred = gsearch1.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f_score = 2*precision*recall/(precision+recall)\n",
    "print(\"F1 score\", f_score)\n",
    "print_confusion_matrix(tn, fp, fn, tp)\n",
    "print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_params = {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.001, 'max_delta_step': 0, 'max_depth': 4, 'min_child_weight': 3, 'n_estimators': 50, 'reg_alpha': 0.01, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
    "\n",
    "\n",
    "#best_params = {'colsample_bytree': 0.9000000000000001,\n",
    "#  'gamma': 0,\n",
    "#  'learning_rate': 0.01,\n",
    "#  'max_delta_step': 1,\n",
    "#  'max_depth': 5,\n",
    "#  'min_child_weight': 5,\n",
    "#  'n_estimators': 90,\n",
    "#  'scale_pos_weight': 10,\n",
    "#  'subsample': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(val, pos):\n",
    "    if val == 0:\n",
    "        return \"BULL\"\n",
    "    if val == 1:\n",
    "        return \"BEAR\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(real, imputed):\n",
    "    fig, ax = plt.subplots(figsize=(20,4))\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(50))\n",
    "    ax.plot(real.index, real, '-', label=\"Real Data\", color='blue')\n",
    "    ax.plot(real.index, imputed, '-', label=\"Imputed Data\", color='red')\n",
    "    ax.legend(['Real Data', 'Predicted Data'])\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(format))\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_multiple_times(best_params, n=30):\n",
    "    result_dict = {}\n",
    "    result_dict= {'f1':0, 'tn':0, 'fp':0, 'fn':0, 'tp':0, 'auc':0, 'best_f1':0, 'best_model':None, 'best_auc':0, 'best_f1':0, 'best_tn':0, 'best_fp':0, 'best_fn':0, 'best_tp':0}\n",
    "    X_train = read_csv('output/\\CompleteIndexesWeeklyTrain.csv')\n",
    "    X_test = read_csv('output/\\CompleteIndexesWeeklyTest.csv')\n",
    "\n",
    "\n",
    "    y = pd.read_csv('input/\\sp500_target_regimes.csv', parse_dates=True)\n",
    "    y.index = y['date'].values\n",
    "    y = y[['regime']]\n",
    "    y = y['regime']=='BEAR'\n",
    "    y = pd.DataFrame (y, columns = ['regime'])\n",
    "\n",
    "    y_train = y.loc['2000-01-01':'2015-01-01']\n",
    "    y_test = y.loc['2015-01-02':]\n",
    "\n",
    "    for i in range(0,n):\n",
    "        balance = np.sqrt(len(y_train[y_train['regime']==False])/len(y_train[y_train['regime']==True]))\n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                          #eval_metric = 'auc',\n",
    "                                          learning_rate=best_params['learning_rate'], \n",
    "                                          n_estimators=best_params['n_estimators'], \n",
    "                                          min_child_weight=best_params['min_child_weight'], \n",
    "                                          gamma=best_params['gamma'],\n",
    "                                          max_delta_step=best_params['max_delta_step'],\n",
    "                                          max_depth=best_params['max_depth'],\n",
    "                                          subsample=best_params['subsample'],\n",
    "                                          scale_pos_weight=best_params['scale_pos_weight'],\n",
    "                                          colsample_bytree=best_params['colsample_bytree'],\n",
    "                                          reg_alpha=best_params['reg_alpha']\n",
    "                                      )\n",
    "\n",
    "        model = xgb_model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f_score = (2*precision*recall/(precision+recall))\n",
    "        result_dict['f1'] += f_score\n",
    "        result_dict['tn'] += tn\n",
    "        result_dict['fp'] += fp\n",
    "        result_dict['fn'] += fn\n",
    "        result_dict['tp'] += tp\n",
    "        result_dict['auc'] += gsearch1.best_score_\n",
    "        if result_dict['best_f1'] < f_score:\n",
    "            result_dict['best_f1'] = f_score\n",
    "            result_dict['best_model'] = model\n",
    "            result_dict['best_tn'] = tn\n",
    "            result_dict['best_fp'] = fp\n",
    "            result_dict['best_fn'] = fn\n",
    "            result_dict['best_tp'] = tp\n",
    "            result_dict['best_auc'] = gsearch1.best_score_\n",
    "            result_dict['best_params'] = gsearch1.best_params_\n",
    "        \n",
    "\n",
    "    #plot bear bull imputed and real\n",
    "    plot(y_test, y_pred)\n",
    "    # feature importance\n",
    "    data = pd.DataFrame(data=xgb_model.feature_importances_, index=X_train.columns, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "    data.plot(kind='barh', title=\"Predicted v. Real\", figsize=(10,10))\n",
    "\n",
    "    result_dict['f1'] = result_dict['f1']/n\n",
    "    result_dict['tn'] = result_dict['tn']/n\n",
    "    result_dict['fp'] = result_dict['fp']/n\n",
    "    result_dict['fn'] = result_dict['fn']/n\n",
    "    result_dict['tp'] = result_dict['tp']/n\n",
    "    result_dict['auc'] = result_dict['auc']/n\n",
    "        \n",
    "    return result_dict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_dict = run_experiment_multiple_times(gsearch1.best_params_, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average F1 Score\", result_dict['f1'])\n",
    "print_confusion_matrix(result_dict['tn'], result_dict['fp'], result_dict['fn'], result_dict['tp'])\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(\"Best F1 Score\", result_dict['best_f1'])\n",
    "print_confusion_matrix(result_dict['best_tn'], result_dict['best_fp'], result_dict['best_fn'], result_dict['best_tp'])\n",
    "print(\"_______________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
